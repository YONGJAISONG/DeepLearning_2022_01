{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "송용재_0.8492.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 타이타닉 데이터 이진분류 정확도 경시대회\n",
        "- Seaborn titanic data : 전처리는 ML에서 했던대로 할 것\n",
        "- random seed = 2022\n",
        "- train_test_split : test_size = 0.2\n",
        "- validation_split = 0.2\n",
        "\n",
        "신경망을 사용해서 정확도를 도출  \n",
        "모델 정의/설정/실행/학습 --> 임의로 결정\n",
        "\n",
        "파일 이름 : 이름 0.8934.html의 형식으로"
      ],
      "metadata": {
        "id": "zadwSkAjoy1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG3Zelt-ouoe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 필요한 모듈 불러오기 및 데이터 로딩"
      ],
      "metadata": {
        "id": "p-vpecpwwsNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# 관습적으로 쓰기도 한다\n",
        "seed = 2022\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "hJq-NuHHwyQj"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset('titanic')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SNYt1oKhyTeJ",
        "outputId": "66ae163b-6076-4aa4-9529-b28109dadd00"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d5d3eaf9-d631-4788-9819-461959b9a236\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>class</th>\n",
              "      <th>who</th>\n",
              "      <th>adult_male</th>\n",
              "      <th>deck</th>\n",
              "      <th>embark_town</th>\n",
              "      <th>alive</th>\n",
              "      <th>alone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5d3eaf9-d631-4788-9819-461959b9a236')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5d3eaf9-d631-4788-9819-461959b9a236 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5d3eaf9-d631-4788-9819-461959b9a236');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
              "0         0       3    male  22.0  ...   NaN  Southampton     no  False\n",
              "1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
              "2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
              "3         1       1  female  35.0  ...     C  Southampton    yes  False\n",
              "4         0       3    male  35.0  ...   NaN  Southampton     no   True\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 전처리"
      ],
      "metadata": {
        "id": "QR5q60mz0YXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 필요한 컬럼 추리기"
      ],
      "metadata": {
        "id": "VOMOPLsx0cx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'deck']]\n",
        "df.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "V60bqPZ-yhvO",
        "outputId": "7da0ef24-e2d5-44a1-e117-e1adfeba151e"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1fbaf5bd-4f13-4c41-9801-de50d68712c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>deck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.4583</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fbaf5bd-4f13-4c41-9801-de50d68712c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1fbaf5bd-4f13-4c41-9801-de50d68712c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1fbaf5bd-4f13-4c41-9801-de50d68712c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked deck\n",
              "0         0       3    male  22.0      1      0   7.2500        S  NaN\n",
              "1         1       1  female  38.0      1      0  71.2833        C    C\n",
              "2         1       3  female  26.0      0      0   7.9250        S  NaN\n",
              "3         1       1  female  35.0      1      0  53.1000        S    C\n",
              "4         0       3    male  35.0      0      0   8.0500        S  NaN\n",
              "5         0       3    male   NaN      0      0   8.4583        Q  NaN"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 결측값 확인 및 처리"
      ],
      "metadata": {
        "id": "NdwsGQNM0_1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRaFSiuK09KY",
        "outputId": "42e84195-ce0b-4d90-eb39-7ce67fe19848"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "survived      0\n",
              "pclass        0\n",
              "sex           0\n",
              "age         177\n",
              "sibsp         0\n",
              "parch         0\n",
              "fare          0\n",
              "embarked      2\n",
              "deck        688\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### embarked의 경우 최빈값으로 대체"
      ],
      "metadata": {
        "id": "ZYH0NVoY1F7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['embarked'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PTlCJdH1TWz",
        "outputId": "76a863f9-08c3-4034-a321-86bdca393994"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "S    644\n",
              "C    168\n",
              "Q     77\n",
              "Name: embarked, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['embarked'].fillna('S', inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "wh5YVMAM1gjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77716043-e9c5-4396-e270-a86f5ad160f7"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "survived      0\n",
              "pclass        0\n",
              "sex           0\n",
              "age         177\n",
              "sibsp         0\n",
              "parch         0\n",
              "fare          0\n",
              "embarked      0\n",
              "deck        688\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### age의 경우 평균값(소수 첫째자리)으로 대체"
      ],
      "metadata": {
        "id": "zt_EuzSy1x8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(df['age'].mean(), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl1f6RbP16rv",
        "outputId": "ae7a779c-b1c2-40b5-81aa-d366366ff49b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.7"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].fillna(round(df['age'].mean(), 1), inplace=True)\n",
        "df.head(6)"
      ],
      "metadata": {
        "id": "GRpb53Hs19M1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d80a955c-548e-4568-80c5-808328a6311d"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1d413434-ecd9-4de1-b1f2-48812c8fe64a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>deck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>29.7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.4583</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d413434-ecd9-4de1-b1f2-48812c8fe64a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d413434-ecd9-4de1-b1f2-48812c8fe64a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d413434-ecd9-4de1-b1f2-48812c8fe64a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked deck\n",
              "0         0       3    male  22.0      1      0   7.2500        S  NaN\n",
              "1         1       1  female  38.0      1      0  71.2833        C    C\n",
              "2         1       3  female  26.0      0      0   7.9250        S  NaN\n",
              "3         1       1  female  35.0      1      0  53.1000        S    C\n",
              "4         0       3    male  35.0      0      0   8.0500        S  NaN\n",
              "5         0       3    male  29.7      0      0   8.4583        Q  NaN"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### deck 컬럼의 경우 결측치가 너무 많으므로 삭제"
      ],
      "metadata": {
        "id": "QzEZORZS2S2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['deck'], inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7gYBu3TM2e4W",
        "outputId": "b967f5ef-40f1-4b8d-9bc1-a52aa46cd16b"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-94606cde-9078-4b00-8d7a-3f3d7143d845\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94606cde-9078-4b00-8d7a-3f3d7143d845')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94606cde-9078-4b00-8d7a-3f3d7143d845 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94606cde-9078-4b00-8d7a-3f3d7143d845');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked\n",
              "0         0       3    male  22.0      1      0   7.2500        S\n",
              "1         1       1  female  38.0      1      0  71.2833        C\n",
              "2         1       3  female  26.0      0      0   7.9250        S\n",
              "3         1       1  female  35.0      1      0  53.1000        S\n",
              "4         0       3    male  35.0      0      0   8.0500        S"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) 데이터 인코딩 및 스케일링"
      ],
      "metadata": {
        "id": "fNzWo5Gh2mAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUaYBA4w3aVi",
        "outputId": "c5c3a847-972c-4440-85bb-3e018e5d9d04"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 8 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   survived  891 non-null    int64  \n",
            " 1   pclass    891 non-null    int64  \n",
            " 2   sex       891 non-null    object \n",
            " 3   age       891 non-null    float64\n",
            " 4   sibsp     891 non-null    int64  \n",
            " 5   parch     891 non-null    int64  \n",
            " 6   fare      891 non-null    float64\n",
            " 7   embarked  891 non-null    object \n",
            "dtypes: float64(2), int64(4), object(2)\n",
            "memory usage: 55.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 'sex' , 'embarked' 컬럼의 경우 카테고리 값이므로 레이블 인코딩 실시"
      ],
      "metadata": {
        "id": "KQ60zN583SdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(df['sex'])\n",
        "df['sex'] = le.transform(df['sex'])\n",
        "\n",
        "df['embarked'] = le.fit_transform(df['embarked'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ITOOc2x83moh",
        "outputId": "194ed9ec-a9e6-425c-885e-49ebefce6110"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c1e79e11-b540-4dc5-9ae5-bc8dc84d7268\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1e79e11-b540-4dc5-9ae5-bc8dc84d7268')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1e79e11-b540-4dc5-9ae5-bc8dc84d7268 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1e79e11-b540-4dc5-9ae5-bc8dc84d7268');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   survived  pclass  sex   age  sibsp  parch     fare  embarked\n",
              "0         0       3    1  22.0      1      0   7.2500         2\n",
              "1         1       1    0  38.0      1      0  71.2833         0\n",
              "2         1       3    0  26.0      0      0   7.9250         2\n",
              "3         1       1    0  35.0      1      0  53.1000         2\n",
              "4         0       3    1  35.0      0      0   8.0500         2"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### X, y값을 설정"
      ],
      "metadata": {
        "id": "5ElTyrSW5Yy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 1:].values\n",
        "y = df.iloc[:, 0].values"
      ],
      "metadata": {
        "id": "RwquHCFZ5YmI"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### X의 각 컬럼 범위 분포가 서로 다르므로 정규화 스케일링 실시"
      ],
      "metadata": {
        "id": "Sq0Ij1RM4r_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_std = scaler.transform(X)"
      ],
      "metadata": {
        "id": "nAkRveRx5KIT"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train/Test 분리"
      ],
      "metadata": {
        "id": "92wqYBH16HiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_std, y, stratify = y, test_size = 0.2, random_state = 2022\n",
        ")\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsfoQVUX571W",
        "outputId": "801099ca-96be-4ce0-8a8e-dcb62c44fa2f"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((712, 7), (179, 7), (712,), (179,))"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model 설정"
      ],
      "metadata": {
        "id": "25qyYDUA65bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "NgfP6r267FWS"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "                    Dense(64, input_dim = 7, activation = 'relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(16, activation = 'relu'),\n",
        "                    Dense(8, activation = 'relu'),\n",
        "                    Dense(4, activation = 'relu'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAmHlYn17N59",
        "outputId": "ea5b3bbe-133f-4d97-97a8-9deac554ac46"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_296 (Dense)           (None, 64)                512       \n",
            "                                                                 \n",
            " dense_297 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_298 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_299 (Dense)           (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_300 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_301 (Dense)           (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,297\n",
            "Trainable params: 3,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "hist = model.fit(X_train, y_train, validation_split=0.2, batch_size=50, epochs=200,\n",
        "                 verbose = 1, callbacks = [checkpoint])"
      ],
      "metadata": {
        "id": "n2MioDKm8A0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6825ef51-8d08-49b4-c277-f4969074bac8"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            " 1/12 [=>............................] - ETA: 5s - loss: 0.7089 - accuracy: 0.6000\n",
            "Epoch 00001: val_loss did not improve from 0.53542\n",
            "12/12 [==============================] - 1s 18ms/step - loss: 0.6789 - accuracy: 0.6327 - val_loss: 0.6706 - val_accuracy: 0.6014\n",
            "Epoch 2/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6373 - accuracy: 0.7400\n",
            "Epoch 00002: val_loss did not improve from 0.53542\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6420 - accuracy: 0.6801 - val_loss: 0.6381 - val_accuracy: 0.6993\n",
            "Epoch 3/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6444 - accuracy: 0.5800\n",
            "Epoch 00003: val_loss did not improve from 0.53542\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5985 - accuracy: 0.7311 - val_loss: 0.5999 - val_accuracy: 0.7483\n",
            "Epoch 4/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5784 - accuracy: 0.7800\n",
            "Epoch 00004: val_loss did not improve from 0.53542\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5474 - accuracy: 0.7768 - val_loss: 0.5545 - val_accuracy: 0.7413\n",
            "Epoch 5/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5699 - accuracy: 0.7400\n",
            "Epoch 00005: val_loss improved from 0.53542 to 0.51226, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4913 - accuracy: 0.8207 - val_loss: 0.5123 - val_accuracy: 0.7692\n",
            "Epoch 6/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5950 - accuracy: 0.7000\n",
            "Epoch 00006: val_loss improved from 0.51226 to 0.49134, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4470 - accuracy: 0.8172 - val_loss: 0.4913 - val_accuracy: 0.7692\n",
            "Epoch 7/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3636 - accuracy: 0.8800\n",
            "Epoch 00007: val_loss did not improve from 0.49134\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.8243 - val_loss: 0.4921 - val_accuracy: 0.7552\n",
            "Epoch 8/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4704 - accuracy: 0.7800\n",
            "Epoch 00008: val_loss improved from 0.49134 to 0.47976, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4127 - accuracy: 0.8207 - val_loss: 0.4798 - val_accuracy: 0.7762\n",
            "Epoch 9/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2638 - accuracy: 0.9000\n",
            "Epoch 00009: val_loss improved from 0.47976 to 0.47403, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4031 - accuracy: 0.8260 - val_loss: 0.4740 - val_accuracy: 0.7832\n",
            "Epoch 10/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3740 - accuracy: 0.8400\n",
            "Epoch 00010: val_loss improved from 0.47403 to 0.46742, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.8295 - val_loss: 0.4674 - val_accuracy: 0.7902\n",
            "Epoch 11/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4723 - accuracy: 0.7200\n",
            "Epoch 00011: val_loss did not improve from 0.46742\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3894 - accuracy: 0.8313 - val_loss: 0.4676 - val_accuracy: 0.7902\n",
            "Epoch 12/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4704 - accuracy: 0.7800\n",
            "Epoch 00012: val_loss improved from 0.46742 to 0.46489, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3854 - accuracy: 0.8313 - val_loss: 0.4649 - val_accuracy: 0.7902\n",
            "Epoch 13/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3328 - accuracy: 0.8800\n",
            "Epoch 00013: val_loss improved from 0.46489 to 0.46308, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3820 - accuracy: 0.8348 - val_loss: 0.4631 - val_accuracy: 0.7832\n",
            "Epoch 14/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4234 - accuracy: 0.7800\n",
            "Epoch 00014: val_loss improved from 0.46308 to 0.45774, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3774 - accuracy: 0.8453 - val_loss: 0.4577 - val_accuracy: 0.7902\n",
            "Epoch 15/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4030 - accuracy: 0.7800\n",
            "Epoch 00015: val_loss did not improve from 0.45774\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8366 - val_loss: 0.4588 - val_accuracy: 0.7972\n",
            "Epoch 16/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4298 - accuracy: 0.8000\n",
            "Epoch 00016: val_loss did not improve from 0.45774\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3719 - accuracy: 0.8401 - val_loss: 0.4612 - val_accuracy: 0.8042\n",
            "Epoch 17/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3797 - accuracy: 0.8400\n",
            "Epoch 00017: val_loss did not improve from 0.45774\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3641 - accuracy: 0.8471 - val_loss: 0.4639 - val_accuracy: 0.7692\n",
            "Epoch 18/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3218 - accuracy: 0.9000\n",
            "Epoch 00018: val_loss improved from 0.45774 to 0.45718, saving model to model/best_titanic6.h5\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3618 - accuracy: 0.8471 - val_loss: 0.4572 - val_accuracy: 0.8112\n",
            "Epoch 19/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2585 - accuracy: 0.8800\n",
            "Epoch 00019: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3585 - accuracy: 0.8453 - val_loss: 0.4615 - val_accuracy: 0.7832\n",
            "Epoch 20/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2617 - accuracy: 0.9000\n",
            "Epoch 00020: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3548 - accuracy: 0.8541 - val_loss: 0.4623 - val_accuracy: 0.8112\n",
            "Epoch 21/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2477 - accuracy: 0.9000\n",
            "Epoch 00021: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3511 - accuracy: 0.8506 - val_loss: 0.4604 - val_accuracy: 0.7972\n",
            "Epoch 22/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3652 - accuracy: 0.8200\n",
            "Epoch 00022: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8506 - val_loss: 0.4638 - val_accuracy: 0.8112\n",
            "Epoch 23/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2483 - accuracy: 0.9200\n",
            "Epoch 00023: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3486 - accuracy: 0.8524 - val_loss: 0.4601 - val_accuracy: 0.7972\n",
            "Epoch 24/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4030 - accuracy: 0.8000\n",
            "Epoch 00024: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3456 - accuracy: 0.8541 - val_loss: 0.4632 - val_accuracy: 0.8042\n",
            "Epoch 25/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3756 - accuracy: 0.8200\n",
            "Epoch 00025: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.8612 - val_loss: 0.4695 - val_accuracy: 0.7832\n",
            "Epoch 26/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2345 - accuracy: 0.9400\n",
            "Epoch 00026: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3404 - accuracy: 0.8629 - val_loss: 0.4682 - val_accuracy: 0.8112\n",
            "Epoch 27/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2962 - accuracy: 0.8800\n",
            "Epoch 00027: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.8647 - val_loss: 0.4675 - val_accuracy: 0.7902\n",
            "Epoch 28/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3560 - accuracy: 0.8800\n",
            "Epoch 00028: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3360 - accuracy: 0.8612 - val_loss: 0.4642 - val_accuracy: 0.7902\n",
            "Epoch 29/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3832 - accuracy: 0.8400\n",
            "Epoch 00029: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3333 - accuracy: 0.8612 - val_loss: 0.4745 - val_accuracy: 0.8042\n",
            "Epoch 30/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4110 - accuracy: 0.8400\n",
            "Epoch 00030: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3333 - accuracy: 0.8664 - val_loss: 0.4672 - val_accuracy: 0.7902\n",
            "Epoch 31/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2331 - accuracy: 0.9000\n",
            "Epoch 00031: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8612 - val_loss: 0.4692 - val_accuracy: 0.7902\n",
            "Epoch 32/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2928 - accuracy: 0.9200\n",
            "Epoch 00032: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.8787 - val_loss: 0.4762 - val_accuracy: 0.7832\n",
            "Epoch 33/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2498 - accuracy: 0.8800\n",
            "Epoch 00033: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3268 - accuracy: 0.8664 - val_loss: 0.4775 - val_accuracy: 0.7972\n",
            "Epoch 34/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1852 - accuracy: 0.9400\n",
            "Epoch 00034: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8682 - val_loss: 0.4785 - val_accuracy: 0.7762\n",
            "Epoch 35/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4095 - accuracy: 0.8400\n",
            "Epoch 00035: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8717 - val_loss: 0.4804 - val_accuracy: 0.7972\n",
            "Epoch 36/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2494 - accuracy: 0.8800\n",
            "Epoch 00036: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3233 - accuracy: 0.8682 - val_loss: 0.4741 - val_accuracy: 0.7832\n",
            "Epoch 37/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2780 - accuracy: 0.8800\n",
            "Epoch 00037: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8682 - val_loss: 0.4831 - val_accuracy: 0.7972\n",
            "Epoch 38/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3632 - accuracy: 0.8400\n",
            "Epoch 00038: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3211 - accuracy: 0.8682 - val_loss: 0.4794 - val_accuracy: 0.7902\n",
            "Epoch 39/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3192 - accuracy: 0.8800\n",
            "Epoch 00039: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8664 - val_loss: 0.4860 - val_accuracy: 0.7832\n",
            "Epoch 40/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2942 - accuracy: 0.9000\n",
            "Epoch 00040: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.8770 - val_loss: 0.4822 - val_accuracy: 0.7972\n",
            "Epoch 41/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2476 - accuracy: 0.9400\n",
            "Epoch 00041: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3122 - accuracy: 0.8735 - val_loss: 0.4826 - val_accuracy: 0.8042\n",
            "Epoch 42/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8800\n",
            "Epoch 00042: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3105 - accuracy: 0.8735 - val_loss: 0.4855 - val_accuracy: 0.7832\n",
            "Epoch 43/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2313 - accuracy: 0.8800\n",
            "Epoch 00043: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.8717 - val_loss: 0.4858 - val_accuracy: 0.7762\n",
            "Epoch 44/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2732 - accuracy: 0.8800\n",
            "Epoch 00044: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3079 - accuracy: 0.8770 - val_loss: 0.4817 - val_accuracy: 0.8042\n",
            "Epoch 45/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3268 - accuracy: 0.8800\n",
            "Epoch 00045: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3075 - accuracy: 0.8735 - val_loss: 0.4930 - val_accuracy: 0.7692\n",
            "Epoch 46/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2453 - accuracy: 0.9000\n",
            "Epoch 00046: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3060 - accuracy: 0.8805 - val_loss: 0.4919 - val_accuracy: 0.7972\n",
            "Epoch 47/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2106 - accuracy: 0.9200\n",
            "Epoch 00047: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3002 - accuracy: 0.8752 - val_loss: 0.4982 - val_accuracy: 0.7552\n",
            "Epoch 48/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3031 - accuracy: 0.8800\n",
            "Epoch 00048: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.8752 - val_loss: 0.5021 - val_accuracy: 0.8112\n",
            "Epoch 49/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3807 - accuracy: 0.8200\n",
            "Epoch 00049: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3054 - accuracy: 0.8717 - val_loss: 0.5013 - val_accuracy: 0.7902\n",
            "Epoch 50/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2057 - accuracy: 0.9400\n",
            "Epoch 00050: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3011 - accuracy: 0.8858 - val_loss: 0.4890 - val_accuracy: 0.7902\n",
            "Epoch 51/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2635 - accuracy: 0.8800\n",
            "Epoch 00051: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2968 - accuracy: 0.8770 - val_loss: 0.4965 - val_accuracy: 0.7832\n",
            "Epoch 52/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2993 - accuracy: 0.8800\n",
            "Epoch 00052: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3006 - accuracy: 0.8805 - val_loss: 0.5048 - val_accuracy: 0.7832\n",
            "Epoch 53/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3536 - accuracy: 0.8800\n",
            "Epoch 00053: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2953 - accuracy: 0.8858 - val_loss: 0.4940 - val_accuracy: 0.7902\n",
            "Epoch 54/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4394 - accuracy: 0.8000\n",
            "Epoch 00054: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2924 - accuracy: 0.8822 - val_loss: 0.4963 - val_accuracy: 0.7832\n",
            "Epoch 55/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2446 - accuracy: 0.8800\n",
            "Epoch 00055: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2927 - accuracy: 0.8858 - val_loss: 0.5025 - val_accuracy: 0.8182\n",
            "Epoch 56/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2642 - accuracy: 0.8800\n",
            "Epoch 00056: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2952 - accuracy: 0.8875 - val_loss: 0.5075 - val_accuracy: 0.7692\n",
            "Epoch 57/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2211 - accuracy: 0.9400\n",
            "Epoch 00057: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2883 - accuracy: 0.8840 - val_loss: 0.5061 - val_accuracy: 0.7762\n",
            "Epoch 58/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3759 - accuracy: 0.8600\n",
            "Epoch 00058: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2886 - accuracy: 0.8770 - val_loss: 0.5044 - val_accuracy: 0.7902\n",
            "Epoch 59/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2516 - accuracy: 0.9400\n",
            "Epoch 00059: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2873 - accuracy: 0.8875 - val_loss: 0.5125 - val_accuracy: 0.7902\n",
            "Epoch 60/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3423 - accuracy: 0.9000\n",
            "Epoch 00060: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2858 - accuracy: 0.8840 - val_loss: 0.5141 - val_accuracy: 0.7762\n",
            "Epoch 61/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2562 - accuracy: 0.9000\n",
            "Epoch 00061: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2842 - accuracy: 0.8770 - val_loss: 0.5101 - val_accuracy: 0.7762\n",
            "Epoch 62/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2901 - accuracy: 0.9000\n",
            "Epoch 00062: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2895 - accuracy: 0.8805 - val_loss: 0.5132 - val_accuracy: 0.8112\n",
            "Epoch 63/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2998 - accuracy: 0.9000\n",
            "Epoch 00063: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2936 - accuracy: 0.8805 - val_loss: 0.5176 - val_accuracy: 0.7762\n",
            "Epoch 64/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2589 - accuracy: 0.9200\n",
            "Epoch 00064: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2889 - accuracy: 0.8858 - val_loss: 0.5173 - val_accuracy: 0.7832\n",
            "Epoch 65/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1703 - accuracy: 0.9600\n",
            "Epoch 00065: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2889 - accuracy: 0.8858 - val_loss: 0.5207 - val_accuracy: 0.7483\n",
            "Epoch 66/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2758 - accuracy: 0.8600\n",
            "Epoch 00066: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2801 - accuracy: 0.8910 - val_loss: 0.5116 - val_accuracy: 0.8112\n",
            "Epoch 67/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8000\n",
            "Epoch 00067: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2818 - accuracy: 0.8840 - val_loss: 0.5223 - val_accuracy: 0.7832\n",
            "Epoch 68/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2688 - accuracy: 0.8800\n",
            "Epoch 00068: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.8875 - val_loss: 0.5284 - val_accuracy: 0.7762\n",
            "Epoch 69/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2048 - accuracy: 0.9400\n",
            "Epoch 00069: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.8787 - val_loss: 0.5266 - val_accuracy: 0.8112\n",
            "Epoch 70/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4013 - accuracy: 0.7600\n",
            "Epoch 00070: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2869 - accuracy: 0.8735 - val_loss: 0.5273 - val_accuracy: 0.7832\n",
            "Epoch 71/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2560 - accuracy: 0.9200\n",
            "Epoch 00071: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2800 - accuracy: 0.8840 - val_loss: 0.5277 - val_accuracy: 0.7762\n",
            "Epoch 72/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1865 - accuracy: 0.9400\n",
            "Epoch 00072: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2796 - accuracy: 0.8840 - val_loss: 0.5349 - val_accuracy: 0.7762\n",
            "Epoch 73/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3259 - accuracy: 0.8600\n",
            "Epoch 00073: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2914 - accuracy: 0.8752 - val_loss: 0.5306 - val_accuracy: 0.8042\n",
            "Epoch 74/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2469 - accuracy: 0.9000\n",
            "Epoch 00074: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2813 - accuracy: 0.8981 - val_loss: 0.5172 - val_accuracy: 0.7902\n",
            "Epoch 75/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2634 - accuracy: 0.8800\n",
            "Epoch 00075: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2729 - accuracy: 0.8875 - val_loss: 0.5280 - val_accuracy: 0.7832\n",
            "Epoch 76/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3626 - accuracy: 0.8400\n",
            "Epoch 00076: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2694 - accuracy: 0.8893 - val_loss: 0.5347 - val_accuracy: 0.7832\n",
            "Epoch 77/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2926 - accuracy: 0.9000\n",
            "Epoch 00077: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2698 - accuracy: 0.8858 - val_loss: 0.5373 - val_accuracy: 0.7832\n",
            "Epoch 78/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2729 - accuracy: 0.8800\n",
            "Epoch 00078: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2706 - accuracy: 0.8893 - val_loss: 0.5338 - val_accuracy: 0.7902\n",
            "Epoch 79/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3308 - accuracy: 0.8200\n",
            "Epoch 00079: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2698 - accuracy: 0.8822 - val_loss: 0.5453 - val_accuracy: 0.7902\n",
            "Epoch 80/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2525 - accuracy: 0.8600\n",
            "Epoch 00080: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2686 - accuracy: 0.8875 - val_loss: 0.5409 - val_accuracy: 0.7972\n",
            "Epoch 81/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2260 - accuracy: 0.8800\n",
            "Epoch 00081: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2687 - accuracy: 0.8858 - val_loss: 0.5407 - val_accuracy: 0.7832\n",
            "Epoch 82/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2931 - accuracy: 0.9000\n",
            "Epoch 00082: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2667 - accuracy: 0.8928 - val_loss: 0.5380 - val_accuracy: 0.7972\n",
            "Epoch 83/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1680 - accuracy: 0.9600\n",
            "Epoch 00083: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.8717 - val_loss: 0.5434 - val_accuracy: 0.7972\n",
            "Epoch 84/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1810 - accuracy: 0.9600\n",
            "Epoch 00084: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2769 - accuracy: 0.8840 - val_loss: 0.5415 - val_accuracy: 0.7832\n",
            "Epoch 85/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3581 - accuracy: 0.8400\n",
            "Epoch 00085: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2698 - accuracy: 0.8875 - val_loss: 0.5559 - val_accuracy: 0.8042\n",
            "Epoch 86/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1208 - accuracy: 0.9800\n",
            "Epoch 00086: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.8893 - val_loss: 0.5632 - val_accuracy: 0.7832\n",
            "Epoch 87/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2872 - accuracy: 0.8800\n",
            "Epoch 00087: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.9016 - val_loss: 0.5497 - val_accuracy: 0.7972\n",
            "Epoch 88/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1571 - accuracy: 0.9600\n",
            "Epoch 00088: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2647 - accuracy: 0.8928 - val_loss: 0.5588 - val_accuracy: 0.8182\n",
            "Epoch 89/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2504 - accuracy: 0.9200\n",
            "Epoch 00089: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2638 - accuracy: 0.8910 - val_loss: 0.5662 - val_accuracy: 0.7483\n",
            "Epoch 90/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2672 - accuracy: 0.8600\n",
            "Epoch 00090: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2641 - accuracy: 0.8875 - val_loss: 0.5615 - val_accuracy: 0.8112\n",
            "Epoch 91/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2760 - accuracy: 0.8800\n",
            "Epoch 00091: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.8963 - val_loss: 0.5477 - val_accuracy: 0.7902\n",
            "Epoch 92/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1939 - accuracy: 0.9400\n",
            "Epoch 00092: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.8963 - val_loss: 0.5647 - val_accuracy: 0.7972\n",
            "Epoch 93/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4180 - accuracy: 0.8600\n",
            "Epoch 00093: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.8981 - val_loss: 0.5734 - val_accuracy: 0.7832\n",
            "Epoch 94/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1779 - accuracy: 0.9400\n",
            "Epoch 00094: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.8928 - val_loss: 0.5637 - val_accuracy: 0.7762\n",
            "Epoch 95/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2823 - accuracy: 0.8800\n",
            "Epoch 00095: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2585 - accuracy: 0.8981 - val_loss: 0.5679 - val_accuracy: 0.7972\n",
            "Epoch 96/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2063 - accuracy: 0.8800\n",
            "Epoch 00096: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2570 - accuracy: 0.8875 - val_loss: 0.5886 - val_accuracy: 0.7692\n",
            "Epoch 97/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2189 - accuracy: 0.9000\n",
            "Epoch 00097: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2592 - accuracy: 0.8910 - val_loss: 0.5730 - val_accuracy: 0.8112\n",
            "Epoch 98/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1749 - accuracy: 0.9800\n",
            "Epoch 00098: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2655 - accuracy: 0.8928 - val_loss: 0.5730 - val_accuracy: 0.7692\n",
            "Epoch 99/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3670 - accuracy: 0.8600\n",
            "Epoch 00099: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2848 - accuracy: 0.8910 - val_loss: 0.5787 - val_accuracy: 0.7832\n",
            "Epoch 100/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3526 - accuracy: 0.8800\n",
            "Epoch 00100: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2661 - accuracy: 0.8946 - val_loss: 0.5939 - val_accuracy: 0.8182\n",
            "Epoch 101/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3336 - accuracy: 0.8600\n",
            "Epoch 00101: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.8981 - val_loss: 0.5726 - val_accuracy: 0.7832\n",
            "Epoch 102/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2136 - accuracy: 0.9200\n",
            "Epoch 00102: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2539 - accuracy: 0.8893 - val_loss: 0.5761 - val_accuracy: 0.8042\n",
            "Epoch 103/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2571 - accuracy: 0.9200\n",
            "Epoch 00103: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2541 - accuracy: 0.8946 - val_loss: 0.5875 - val_accuracy: 0.7692\n",
            "Epoch 104/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2554 - accuracy: 0.9000\n",
            "Epoch 00104: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2603 - accuracy: 0.8946 - val_loss: 0.5886 - val_accuracy: 0.7832\n",
            "Epoch 105/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1611 - accuracy: 0.9400\n",
            "Epoch 00105: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.8893 - val_loss: 0.5792 - val_accuracy: 0.7972\n",
            "Epoch 106/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1142 - accuracy: 0.9400\n",
            "Epoch 00106: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2504 - accuracy: 0.9033 - val_loss: 0.5806 - val_accuracy: 0.7762\n",
            "Epoch 107/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3183 - accuracy: 0.8200\n",
            "Epoch 00107: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2592 - accuracy: 0.8840 - val_loss: 0.6218 - val_accuracy: 0.7972\n",
            "Epoch 108/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3097 - accuracy: 0.8200\n",
            "Epoch 00108: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2625 - accuracy: 0.8928 - val_loss: 0.6116 - val_accuracy: 0.7692\n",
            "Epoch 109/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2214 - accuracy: 0.8800\n",
            "Epoch 00109: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2513 - accuracy: 0.9016 - val_loss: 0.5853 - val_accuracy: 0.7902\n",
            "Epoch 110/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2478 - accuracy: 0.9000\n",
            "Epoch 00110: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2494 - accuracy: 0.8946 - val_loss: 0.6053 - val_accuracy: 0.7832\n",
            "Epoch 111/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3588 - accuracy: 0.8400\n",
            "Epoch 00111: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9033 - val_loss: 0.6156 - val_accuracy: 0.8042\n",
            "Epoch 112/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.8200\n",
            "Epoch 00112: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2545 - accuracy: 0.8963 - val_loss: 0.6174 - val_accuracy: 0.7552\n",
            "Epoch 113/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2876 - accuracy: 0.8600\n",
            "Epoch 00113: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2500 - accuracy: 0.9033 - val_loss: 0.6062 - val_accuracy: 0.7832\n",
            "Epoch 114/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2586 - accuracy: 0.9200\n",
            "Epoch 00114: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2488 - accuracy: 0.9033 - val_loss: 0.6167 - val_accuracy: 0.7902\n",
            "Epoch 115/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3180 - accuracy: 0.8200\n",
            "Epoch 00115: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2483 - accuracy: 0.8981 - val_loss: 0.5997 - val_accuracy: 0.7762\n",
            "Epoch 116/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2076 - accuracy: 0.9000\n",
            "Epoch 00116: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2517 - accuracy: 0.9016 - val_loss: 0.6097 - val_accuracy: 0.7902\n",
            "Epoch 117/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1718 - accuracy: 0.9600\n",
            "Epoch 00117: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2415 - accuracy: 0.9104 - val_loss: 0.6113 - val_accuracy: 0.7692\n",
            "Epoch 118/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1666 - accuracy: 0.9400\n",
            "Epoch 00118: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2426 - accuracy: 0.8928 - val_loss: 0.6161 - val_accuracy: 0.7622\n",
            "Epoch 119/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3005 - accuracy: 0.8200\n",
            "Epoch 00119: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2426 - accuracy: 0.9016 - val_loss: 0.6169 - val_accuracy: 0.7902\n",
            "Epoch 120/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2025 - accuracy: 0.9000\n",
            "Epoch 00120: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2441 - accuracy: 0.9033 - val_loss: 0.6383 - val_accuracy: 0.8042\n",
            "Epoch 121/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2496 - accuracy: 0.8800\n",
            "Epoch 00121: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2465 - accuracy: 0.8928 - val_loss: 0.6117 - val_accuracy: 0.7902\n",
            "Epoch 122/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1322 - accuracy: 0.9600\n",
            "Epoch 00122: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2377 - accuracy: 0.9051 - val_loss: 0.6297 - val_accuracy: 0.7692\n",
            "Epoch 123/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2781 - accuracy: 0.9000\n",
            "Epoch 00123: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 0.9121 - val_loss: 0.6424 - val_accuracy: 0.8112\n",
            "Epoch 124/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1633 - accuracy: 0.9600\n",
            "Epoch 00124: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2494 - accuracy: 0.8963 - val_loss: 0.6370 - val_accuracy: 0.7762\n",
            "Epoch 125/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2332 - accuracy: 0.9000\n",
            "Epoch 00125: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2439 - accuracy: 0.9086 - val_loss: 0.6408 - val_accuracy: 0.7832\n",
            "Epoch 126/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9200\n",
            "Epoch 00126: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9086 - val_loss: 0.6331 - val_accuracy: 0.8042\n",
            "Epoch 127/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2400 - accuracy: 0.9000\n",
            "Epoch 00127: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2324 - accuracy: 0.9086 - val_loss: 0.6369 - val_accuracy: 0.7762\n",
            "Epoch 128/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2190 - accuracy: 0.9200\n",
            "Epoch 00128: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.9139 - val_loss: 0.6539 - val_accuracy: 0.7972\n",
            "Epoch 129/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2545 - accuracy: 0.9000\n",
            "Epoch 00129: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2414 - accuracy: 0.8981 - val_loss: 0.6434 - val_accuracy: 0.7762\n",
            "Epoch 130/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2045 - accuracy: 0.9200\n",
            "Epoch 00130: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2397 - accuracy: 0.9033 - val_loss: 0.6474 - val_accuracy: 0.7762\n",
            "Epoch 131/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2125 - accuracy: 0.8600\n",
            "Epoch 00131: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2350 - accuracy: 0.9069 - val_loss: 0.6468 - val_accuracy: 0.7762\n",
            "Epoch 132/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2209 - accuracy: 0.9000\n",
            "Epoch 00132: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2503 - accuracy: 0.8946 - val_loss: 0.6691 - val_accuracy: 0.7972\n",
            "Epoch 133/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1814 - accuracy: 0.9200\n",
            "Epoch 00133: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2560 - accuracy: 0.8946 - val_loss: 0.6477 - val_accuracy: 0.7552\n",
            "Epoch 134/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1799 - accuracy: 0.9400\n",
            "Epoch 00134: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2523 - accuracy: 0.8998 - val_loss: 0.6559 - val_accuracy: 0.8042\n",
            "Epoch 135/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2862 - accuracy: 0.8800\n",
            "Epoch 00135: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2460 - accuracy: 0.9033 - val_loss: 0.6494 - val_accuracy: 0.8042\n",
            "Epoch 136/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1085 - accuracy: 1.0000\n",
            "Epoch 00136: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2430 - accuracy: 0.8998 - val_loss: 0.6726 - val_accuracy: 0.7692\n",
            "Epoch 137/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1668 - accuracy: 0.9600\n",
            "Epoch 00137: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2443 - accuracy: 0.9086 - val_loss: 0.6691 - val_accuracy: 0.7762\n",
            "Epoch 138/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1781 - accuracy: 0.9400\n",
            "Epoch 00138: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2387 - accuracy: 0.9016 - val_loss: 0.6552 - val_accuracy: 0.7902\n",
            "Epoch 139/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2003 - accuracy: 0.9000\n",
            "Epoch 00139: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2402 - accuracy: 0.9016 - val_loss: 0.6626 - val_accuracy: 0.7692\n",
            "Epoch 140/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1727 - accuracy: 0.9400\n",
            "Epoch 00140: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9051 - val_loss: 0.6602 - val_accuracy: 0.7972\n",
            "Epoch 141/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1738 - accuracy: 0.9000\n",
            "Epoch 00141: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.9033 - val_loss: 0.6809 - val_accuracy: 0.7692\n",
            "Epoch 142/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2808 - accuracy: 0.8600\n",
            "Epoch 00142: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.9033 - val_loss: 0.6728 - val_accuracy: 0.8112\n",
            "Epoch 143/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3012 - accuracy: 0.8800\n",
            "Epoch 00143: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2401 - accuracy: 0.8928 - val_loss: 0.6813 - val_accuracy: 0.7762\n",
            "Epoch 144/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2508 - accuracy: 0.9000\n",
            "Epoch 00144: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2217 - accuracy: 0.9104 - val_loss: 0.6806 - val_accuracy: 0.8042\n",
            "Epoch 145/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2438 - accuracy: 0.9000\n",
            "Epoch 00145: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9069 - val_loss: 0.6648 - val_accuracy: 0.8042\n",
            "Epoch 146/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1696 - accuracy: 0.9400\n",
            "Epoch 00146: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8946 - val_loss: 0.6815 - val_accuracy: 0.7972\n",
            "Epoch 147/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1649 - accuracy: 0.9400\n",
            "Epoch 00147: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2359 - accuracy: 0.8928 - val_loss: 0.6842 - val_accuracy: 0.7902\n",
            "Epoch 148/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3008 - accuracy: 0.8800\n",
            "Epoch 00148: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2444 - accuracy: 0.9051 - val_loss: 0.7057 - val_accuracy: 0.7413\n",
            "Epoch 149/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2386 - accuracy: 0.8800\n",
            "Epoch 00149: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2568 - accuracy: 0.8910 - val_loss: 0.6917 - val_accuracy: 0.7972\n",
            "Epoch 150/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3088 - accuracy: 0.8000\n",
            "Epoch 00150: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9051 - val_loss: 0.7061 - val_accuracy: 0.7622\n",
            "Epoch 151/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1128 - accuracy: 0.9800\n",
            "Epoch 00151: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2243 - accuracy: 0.9104 - val_loss: 0.6903 - val_accuracy: 0.8182\n",
            "Epoch 152/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3324 - accuracy: 0.8800\n",
            "Epoch 00152: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.8963 - val_loss: 0.7267 - val_accuracy: 0.7552\n",
            "Epoch 153/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2375 - accuracy: 0.9400\n",
            "Epoch 00153: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2269 - accuracy: 0.9016 - val_loss: 0.7048 - val_accuracy: 0.7972\n",
            "Epoch 154/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2321 - accuracy: 0.9000\n",
            "Epoch 00154: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9051 - val_loss: 0.7136 - val_accuracy: 0.7762\n",
            "Epoch 155/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2477 - accuracy: 0.8600\n",
            "Epoch 00155: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9033 - val_loss: 0.6839 - val_accuracy: 0.8042\n",
            "Epoch 156/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2305 - accuracy: 0.8800\n",
            "Epoch 00156: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2304 - accuracy: 0.8981 - val_loss: 0.7122 - val_accuracy: 0.7622\n",
            "Epoch 157/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2423 - accuracy: 0.8800\n",
            "Epoch 00157: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2298 - accuracy: 0.8981 - val_loss: 0.6922 - val_accuracy: 0.7902\n",
            "Epoch 158/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2027 - accuracy: 0.9200\n",
            "Epoch 00158: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2223 - accuracy: 0.9069 - val_loss: 0.7081 - val_accuracy: 0.7972\n",
            "Epoch 159/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2278 - accuracy: 0.9200\n",
            "Epoch 00159: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2224 - accuracy: 0.9033 - val_loss: 0.6988 - val_accuracy: 0.7972\n",
            "Epoch 160/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2663 - accuracy: 0.8800\n",
            "Epoch 00160: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2244 - accuracy: 0.9033 - val_loss: 0.7285 - val_accuracy: 0.7622\n",
            "Epoch 161/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1842 - accuracy: 0.9600\n",
            "Epoch 00161: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2205 - accuracy: 0.9086 - val_loss: 0.7302 - val_accuracy: 0.8112\n",
            "Epoch 162/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1162 - accuracy: 0.9600\n",
            "Epoch 00162: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2155 - accuracy: 0.9051 - val_loss: 0.7047 - val_accuracy: 0.7902\n",
            "Epoch 163/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2244 - accuracy: 0.8800\n",
            "Epoch 00163: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.8981 - val_loss: 0.7382 - val_accuracy: 0.7762\n",
            "Epoch 164/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2444 - accuracy: 0.9000\n",
            "Epoch 00164: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2170 - accuracy: 0.9104 - val_loss: 0.7174 - val_accuracy: 0.7902\n",
            "Epoch 165/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1600 - accuracy: 0.9200\n",
            "Epoch 00165: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9086 - val_loss: 0.7462 - val_accuracy: 0.7902\n",
            "Epoch 166/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2390 - accuracy: 0.9400\n",
            "Epoch 00166: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2213 - accuracy: 0.9104 - val_loss: 0.7334 - val_accuracy: 0.7832\n",
            "Epoch 167/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2074 - accuracy: 0.9000\n",
            "Epoch 00167: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9016 - val_loss: 0.7504 - val_accuracy: 0.7692\n",
            "Epoch 168/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2372 - accuracy: 0.9200\n",
            "Epoch 00168: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2335 - accuracy: 0.8963 - val_loss: 0.7358 - val_accuracy: 0.7552\n",
            "Epoch 169/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2477 - accuracy: 0.9000\n",
            "Epoch 00169: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9086 - val_loss: 0.7598 - val_accuracy: 0.7832\n",
            "Epoch 170/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2645 - accuracy: 0.8800\n",
            "Epoch 00170: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 0.9086 - val_loss: 0.7610 - val_accuracy: 0.7762\n",
            "Epoch 171/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1008 - accuracy: 0.9600\n",
            "Epoch 00171: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.9086 - val_loss: 0.7290 - val_accuracy: 0.7902\n",
            "Epoch 172/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1771 - accuracy: 0.9200\n",
            "Epoch 00172: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2160 - accuracy: 0.9033 - val_loss: 0.7772 - val_accuracy: 0.7832\n",
            "Epoch 173/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2962 - accuracy: 0.9000\n",
            "Epoch 00173: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2147 - accuracy: 0.9069 - val_loss: 0.7533 - val_accuracy: 0.7692\n",
            "Epoch 174/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1972 - accuracy: 0.9000\n",
            "Epoch 00174: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2158 - accuracy: 0.9051 - val_loss: 0.7599 - val_accuracy: 0.8042\n",
            "Epoch 175/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1531 - accuracy: 0.9600\n",
            "Epoch 00175: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2188 - accuracy: 0.9069 - val_loss: 0.7753 - val_accuracy: 0.7552\n",
            "Epoch 176/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3810 - accuracy: 0.8600\n",
            "Epoch 00176: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2199 - accuracy: 0.9086 - val_loss: 0.7805 - val_accuracy: 0.8042\n",
            "Epoch 177/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3033 - accuracy: 0.8400\n",
            "Epoch 00177: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9016 - val_loss: 0.7716 - val_accuracy: 0.7832\n",
            "Epoch 178/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2523 - accuracy: 0.9000\n",
            "Epoch 00178: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9033 - val_loss: 0.7562 - val_accuracy: 0.7832\n",
            "Epoch 179/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2496 - accuracy: 0.8600\n",
            "Epoch 00179: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9069 - val_loss: 0.7704 - val_accuracy: 0.7972\n",
            "Epoch 180/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2299 - accuracy: 0.8800\n",
            "Epoch 00180: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2154 - accuracy: 0.9104 - val_loss: 0.7975 - val_accuracy: 0.7972\n",
            "Epoch 181/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1468 - accuracy: 0.9600\n",
            "Epoch 00181: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9104 - val_loss: 0.7752 - val_accuracy: 0.7832\n",
            "Epoch 182/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1891 - accuracy: 0.9200\n",
            "Epoch 00182: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9069 - val_loss: 0.7754 - val_accuracy: 0.8042\n",
            "Epoch 183/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1964 - accuracy: 0.9000\n",
            "Epoch 00183: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9069 - val_loss: 0.7709 - val_accuracy: 0.7972\n",
            "Epoch 184/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2050 - accuracy: 0.9200\n",
            "Epoch 00184: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9033 - val_loss: 0.8130 - val_accuracy: 0.7622\n",
            "Epoch 185/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1611 - accuracy: 0.9600\n",
            "Epoch 00185: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2140 - accuracy: 0.9069 - val_loss: 0.7924 - val_accuracy: 0.7832\n",
            "Epoch 186/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1153 - accuracy: 0.9600\n",
            "Epoch 00186: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9086 - val_loss: 0.7912 - val_accuracy: 0.7902\n",
            "Epoch 187/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1937 - accuracy: 0.9200\n",
            "Epoch 00187: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9121 - val_loss: 0.7882 - val_accuracy: 0.7832\n",
            "Epoch 188/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1760 - accuracy: 0.9200\n",
            "Epoch 00188: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2113 - accuracy: 0.9069 - val_loss: 0.8241 - val_accuracy: 0.7832\n",
            "Epoch 189/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2319 - accuracy: 0.9200\n",
            "Epoch 00189: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2086 - accuracy: 0.9069 - val_loss: 0.8193 - val_accuracy: 0.7762\n",
            "Epoch 190/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1662 - accuracy: 0.9400\n",
            "Epoch 00190: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2188 - accuracy: 0.9086 - val_loss: 0.8290 - val_accuracy: 0.7483\n",
            "Epoch 191/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3277 - accuracy: 0.8400\n",
            "Epoch 00191: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.8998 - val_loss: 0.8480 - val_accuracy: 0.8042\n",
            "Epoch 192/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2552 - accuracy: 0.8800\n",
            "Epoch 00192: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2221 - accuracy: 0.8998 - val_loss: 0.8511 - val_accuracy: 0.7413\n",
            "Epoch 193/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3307 - accuracy: 0.8400\n",
            "Epoch 00193: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2224 - accuracy: 0.9016 - val_loss: 0.7892 - val_accuracy: 0.7972\n",
            "Epoch 194/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2105 - accuracy: 0.9400\n",
            "Epoch 00194: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2110 - accuracy: 0.9033 - val_loss: 0.8156 - val_accuracy: 0.8042\n",
            "Epoch 195/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1602 - accuracy: 0.9400\n",
            "Epoch 00195: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9051 - val_loss: 0.7902 - val_accuracy: 0.7902\n",
            "Epoch 196/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2338 - accuracy: 0.9200\n",
            "Epoch 00196: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2163 - accuracy: 0.8998 - val_loss: 0.8292 - val_accuracy: 0.7552\n",
            "Epoch 197/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2413 - accuracy: 0.8800\n",
            "Epoch 00197: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9051 - val_loss: 0.7945 - val_accuracy: 0.7762\n",
            "Epoch 198/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1893 - accuracy: 0.9200\n",
            "Epoch 00198: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.8998 - val_loss: 0.7926 - val_accuracy: 0.8042\n",
            "Epoch 199/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1327 - accuracy: 0.9200\n",
            "Epoch 00199: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2306 - accuracy: 0.9033 - val_loss: 0.8010 - val_accuracy: 0.7902\n",
            "Epoch 200/200\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3506 - accuracy: 0.8400\n",
            "Epoch 00200: val_loss did not improve from 0.45718\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9121 - val_loss: 0.8557 - val_accuracy: 0.7552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('model'):\n",
        "    os.mkdir('model')"
      ],
      "metadata": {
        "id": "mGsrBiFn8zz3"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = 'model/best_titanic.h5'\n",
        "model_path = 'model/titanic_{epoch:03d}_{val_loss:.4f}.h5'"
      ],
      "metadata": {
        "id": "WOP7fSXv9WTg"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path, monitor = 'val_loss', save_best_only=True\n",
        ")\n",
        "\n",
        "# ModelCheckpoint(모델 저장경로, 모델 저장시 기준값, save_best_only여부)"
      ],
      "metadata": {
        "id": "61UE39r19y9e"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model 평가 및 학습과정 시각화"
      ],
      "metadata": {
        "id": "1ryQw59MAB7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "best_model_path = 'model/titanic_023_0.4565.h5'\n",
        "best_model = load_model(best_model_path)\n",
        "best_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YciF6EfA-02i",
        "outputId": "f635354f-9814-457d-f4cb-f9694adde499"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40310925245285034, 0.826815664768219]"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_acc = hist.history['accuracy']\n",
        "y_vloss = hist.history['val_loss']\n",
        "xs = np.arange(1, len(y_acc)+1)"
      ],
      "metadata": {
        "id": "zW-O7i50AtVt"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(xs, y_acc, ms=5, label = 'train accuracy')\n",
        "plt.plot(xs, y_vloss, ms=5, label = 'validation loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "RplpQ-5yA88R",
        "outputId": "02989cbd-fed1-47b7-8997-c64a3f5f7291"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 다른 모델 반복"
      ],
      "metadata": {
        "id": "hDA8Gbz6BznT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential([\n",
        "                    Dense(512, input_dim = 7, activation = 'relu'),\n",
        "                    Dense(256, activation = 'relu'),\n",
        "                    Dense(128, activation = 'relu'),\n",
        "                    Dense(64, activation = 'relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WRBpnvfBUpa",
        "outputId": "d44c201a-06b7-40b0-b284-c913d7df5e36"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_302 (Dense)           (None, 512)               4096      \n",
            "                                                                 \n",
            " dense_303 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_304 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_305 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_306 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_307 (Dense)           (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 178,689\n",
            "Trainable params: 178,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path2 = 'model/best_titanic.h5'\n",
        "# model_path = 'model/titanic_{epoch:03d}_{val_loss:.4f}.h5'\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path2, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "2en68NPTCUsX"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(patience = 40)"
      ],
      "metadata": {
        "id": "S8S-AwfACaVl"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "hist2 = model2.fit(X_train, y_train, batch_size = 200, epochs = 400, validation_split=0.2,\n",
        "                   verbose = 0, callbacks = [checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgFkCdGIB1_n",
        "outputId": "51a4bf4e-7dd4-42c7-fddb-9f3389208636"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.63820, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.63820 to 0.56328, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.56328 to 0.50892, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.50892\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.50892\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.50892 to 0.47558, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.47558 to 0.45558, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.45558 to 0.44630, saving model to model/best_titanic.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.44630\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.44630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = load_model(model_path2)\n",
        "best_model2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc9YIaFoCo9J",
        "outputId": "6a920ee2-df12-463b-9e16-24da771e4225"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4030461311340332, 0.8379888534545898]"
            ]
          },
          "metadata": {},
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential([\n",
        "                    Dense(512, input_dim = 7, activation = 'relu'),\n",
        "                    Dense(256, activation = 'relu'),\n",
        "                    Dense(128, activation = 'relu'),\n",
        "                    Dense(64, activation = 'relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(16, activation = 'relu'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgA9jgmvErTE",
        "outputId": "1b451247-ec4b-4f6c-c3d5-3425e87279ba"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_308 (Dense)           (None, 512)               4096      \n",
            "                                                                 \n",
            " dense_309 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_310 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_311 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_312 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_313 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_314 (Dense)           (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,201\n",
            "Trainable params: 179,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path3 = 'model/best_titanic3.h5'\n",
        "# model_path = 'model/titanic_{epoch:03d}_{val_loss:.4f}.h5'\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path3, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "aPkF_VC2FF2T"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "hist3 = model3.fit(X_train, y_train, batch_size = 200, epochs = 400, validation_split=0.2,\n",
        "                   verbose = 0, callbacks = [checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWuvpYIUFQFk",
        "outputId": "7fd86434-e4f7-4e63-9cec-f6a5a28378b7"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.65078, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.65078 to 0.59271, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.59271 to 0.52651, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.52651 to 0.49144, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.49144\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.49144\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.49144 to 0.47811, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.47811 to 0.46063, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.46063 to 0.45279, saving model to model/best_titanic3.h5\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.45279\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.45279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model3 = load_model(model_path3)\n",
        "best_model3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoUyoO30FW29",
        "outputId": "cec92c9a-7567-47ae-d91f-d7dc00949089"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.40110528469085693, 0.8379888534545898]"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential([\n",
        "                    Dense(64, input_dim = 7, activation = 'relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(16, activation = 'relu'),\n",
        "                    Dense(8, activation = 'relu'),\n",
        "                    Dense(4, activation = 'relu'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTLPjZX_FjuG",
        "outputId": "52586889-e637-4d61-e6f2-6a2abd04d273"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_315 (Dense)           (None, 64)                512       \n",
            "                                                                 \n",
            " dense_316 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_317 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_318 (Dense)           (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_319 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_320 (Dense)           (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,297\n",
            "Trainable params: 3,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path4 = 'model/best_titanic4.h5'\n",
        "# model_path = 'model/titanic_{epoch:03d}_{val_loss:.4f}.h5'\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path4, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "80QODuHNF390"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "hist4 = model4.fit(X_train, y_train, batch_size = 200, epochs = 400, validation_split=0.2,\n",
        "                   verbose = 0, callbacks = [checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZBYac43GHjr",
        "outputId": "a282c13a-7c1b-45b4-8a34-bdc6b54a9183"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.68219, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.68219 to 0.67413, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67413 to 0.66590, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.66590 to 0.65787, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.65787 to 0.64924, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.64924 to 0.64016, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.64016 to 0.63041, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.63041 to 0.61982, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.61982 to 0.60832, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.60832 to 0.59594, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.59594 to 0.58264, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.58264 to 0.56874, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.56874 to 0.55502, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.55502 to 0.54147, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.54147 to 0.52888, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.52888 to 0.51780, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.51780 to 0.50864, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.50864 to 0.50208, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.50208 to 0.49799, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.49799 to 0.49656, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.49656\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.49656 to 0.49545, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.49545 to 0.49280, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.49280 to 0.49023, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.49023 to 0.48802, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.48802 to 0.48619, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.48619 to 0.48531, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.48531\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.48531 to 0.48491, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.48491\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.48491 to 0.48429, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.48429 to 0.48247, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.48247 to 0.48116, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.48116 to 0.48093, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.48093\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.48093\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.48093\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.48093\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.48093 to 0.48092, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.48092 to 0.47857, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.47857 to 0.47710, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.47710 to 0.47596, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.47596\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.47596\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.47596 to 0.47542, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.47542 to 0.47477, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.47477 to 0.47393, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.47393\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.47393\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.47393\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.47393 to 0.47310, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.47310 to 0.47288, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.47288\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.47288 to 0.47235, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.47235\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.47235 to 0.47124, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.47124\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.47124\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.47124 to 0.47003, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.47003 to 0.46942, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.46942 to 0.46848, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.46848\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.46848\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.46848 to 0.46795, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.46795 to 0.46655, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.46655\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.46655\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.46655\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.46655\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.46655 to 0.46652, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.46652 to 0.46628, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.46628\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.46628\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.46628\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.46628\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.46628 to 0.46590, saving model to model/best_titanic4.h5\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.46590\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.46590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model4 = load_model(model_path4)\n",
        "best_model4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZFEOb2AGLre",
        "outputId": "49ebd9eb-e195-40a2-a8cc-6b1a2023d70a"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 4ms/step - loss: 0.4148 - accuracy: 0.8212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4148090183734894, 0.8212290406227112]"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = Sequential([\n",
        "                    Dense(512, input_dim = 7, activation = 'relu'),\n",
        "                    Dense(256, activation = 'relu'),\n",
        "                    Dense(128, activation = 'relu'),\n",
        "                    Dense(64, activation = 'relu'),\n",
        "                    Dense(32, activation = 'relu'),\n",
        "                    Dense(16, activation = 'relu'),\n",
        "                    Dense(8, activation = 'relu'),\n",
        "                    Dense(4, activation = 'relu'),\n",
        "                    Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VhnKE2EGRGH",
        "outputId": "bf431768-6405-46ef-9561-d76a489a7b87"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_321 (Dense)           (None, 512)               4096      \n",
            "                                                                 \n",
            " dense_322 (Dense)           (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_323 (Dense)           (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_324 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_325 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_326 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_327 (Dense)           (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_328 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_329 (Dense)           (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,361\n",
            "Trainable params: 179,361\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path5 = 'model/best_titanic5.h5'\n",
        "# model_path = 'model/titanic_{epoch:03d}_{val_loss:.4f}.h5'\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_path5, monitor = 'val_loss', save_best_only=True, verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "ruUlkOrEGfn6"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "hist5 = model5.fit(X_train, y_train, batch_size = 200, epochs = 400, validation_split=0.2,\n",
        "                   verbose = 0, callbacks = [checkpoint, early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gGebfGaGkUD",
        "outputId": "72419293-f8c7-4e14-beb9-24cea831fe03"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.67376, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.67376 to 0.64163, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.64163 to 0.59676, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.59676 to 0.54534, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.54534 to 0.50889, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.50889 to 0.50808, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.50808 to 0.50172, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.50172 to 0.46879, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.46879\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.46879 to 0.45589, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.45589 to 0.45359, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.45359 to 0.44671, saving model to model/best_titanic5.h5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.44671\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.44671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model5 = load_model(model_path5)\n",
        "best_model5.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb9fij6CGpbD",
        "outputId": "cca3cccc-729a-4fc5-bd0a-c7fcc4bbcbd4"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 2ms/step - loss: 0.4145 - accuracy: 0.8492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4145016372203827, 0.8491619825363159]"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_acc = hist5.history['accuracy']\n",
        "y_vloss = hist5.history['val_loss']\n",
        "xs = np.arange(1, len(y_acc)+1)"
      ],
      "metadata": {
        "id": "F7r9_fdAGu5b"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(xs, y_acc, ms=5, label='train accuracy')\n",
        "plt.plot(xs, y_vloss, ms=5, label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "1abrL8EwKgnx",
        "outputId": "27b2c8e3-1ab8-4ec8-b4ad-6f5db11e85d4"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}